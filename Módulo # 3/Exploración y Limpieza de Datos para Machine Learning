

<h1>MÃ³dulo 3: ExploraciÃ³n y Limpieza de Datos para Machine Learning</h1>

<h2>Objetivo</h2>
<p>En este mÃ³dulo aprenderÃ¡s a explorar, entender y limpiar datos usando herramientas de AWS y Python. Este proceso es crucial para cualquier proyecto de Machine Learning, ya que la calidad de tus datos define directamente el desempeÃ±o de tu modelo.</p>

<hr>

<h2>Â¿QuÃ© es la exploraciÃ³n de datos?</h2>
<p>Explorar datos significa entender cÃ³mo son, quÃ© informaciÃ³n contienen, detectar errores o valores atÃ­picos, y descubrir patrones iniciales. Sin un buen anÃ¡lisis exploratorio, cualquier modelo que entrenes serÃ¡ una caja negra sin contexto.</p>

<h3>Â¿Por quÃ© es clave la exploraciÃ³n?</h3>
<ul>
    <li>âœ… Detectar datos faltantes, duplicados o inconsistentes.</li>
    <li>âœ… Conocer distribuciones, sesgos y valores extremos (outliers).</li>
    <li>âœ… Identificar correlaciones y relaciones entre variables.</li>
    <li>âœ… Evaluar si necesitas transformar o escalar variables.</li>
</ul>

<h3>Herramientas clave</h3>
<p>Para explorar datos, usaremos:</p>
<ul>
    <li>ğŸ“Š Pandas (manipulaciÃ³n de datos).</li>
    <li>ğŸ“ˆ Matplotlib y Seaborn (visualizaciones).</li>
    <li>ğŸ” Amazon Athena (consultas SQL sobre S3).</li>
</ul>

<hr>

<h2>Tipos de Datos</h2>
<p>Identificar correctamente el tipo de dato es fundamental para elegir las tÃ©cnicas de limpieza y visualizaciÃ³n adecuadas:</p>
<ul>
    <li><strong>Estructurados:</strong> Datos tabulares (CSV, Excel, SQL).</li>
    <li><strong>Semi-estructurados:</strong> JSON, XML, logs con formato flexible.</li>
    <li><strong>No estructurados:</strong> ImÃ¡genes, audio, video, texto libre.</li>
    <li><strong>Series temporales:</strong> Datos con componente temporal, como ventas diarias.</li>
</ul>

<hr>

<h2>Distribuciones de Datos</h2>
<p>Conocer la forma de cada variable te ayuda a decidir transformaciones o tÃ©cnicas especiales:</p>
<ul>
    <li>ğŸ“Š <strong>DistribuciÃ³n Normal:</strong> La clÃ¡sica curva de campana.</li>
    <li>ğŸ“‰ <strong>Sesgada:</strong> Mayor concentraciÃ³n de datos en un extremo.</li>
    <li>ğŸ“ˆ <strong>Multimodal:</strong> MÃ¡s de un pico o concentraciÃ³n.</li>
</ul>

<hr>

<h2>Series Temporales: Tendencias y Estacionalidad</h2>
<p>Cuando trabajas con datos de ventas, clima o mÃ©tricas operacionales, es clave identificar:</p>
<ul>
    <li>ğŸ“ˆ <strong>Tendencia:</strong> Crecimiento o caÃ­da general.</li>
    <li>ğŸ”„ <strong>Estacionalidad:</strong> Patrones que se repiten (diarios, mensuales).</li>
</ul>

<hr>

<h2>Primer paso: Cargar datos en un Notebook</h2>
<p>Para explorar datos, primero hay que cargarlos. En AWS, puedes hacerlo de varias formas:</p>
<ul>
    <li>ğŸ“‚ Leer directamente desde un archivo CSV en S3 con Pandas.</li>
    <li>ğŸ“Š Consultar datos directamente desde Redshift o Athena.</li>
    <li>ğŸ”— Usar SageMaker Data Wrangler para cargar, transformar y visualizar datos.</li>
</ul>

<pre><code>
import pandas as pd
import boto3
s3 = boto3.client('s3')
response = s3.get_object(Bucket='ml-curso-datos-limpiados', Key='ventas_procesadas.csv')
df = pd.read_csv(response['Body'])
df.head()
</code></pre>

<hr>

<h2>Chequeo inicial: Evaluar la calidad de los datos</h2>

<p>Antes de comenzar cualquier anÃ¡lisis o entrenamiento, es clave realizar un primer chequeo de calidad sobre el dataset. 
A continuaciÃ³n, te mostramos un ejemplo de dataset y cÃ³mo ejecutar cada uno de los puntos clave usando Pandas.</p>

<h3>ğŸ“Š Ejemplo de dataset (ventas.csv)</h3>
<table border="1" cellpadding="5" cellspacing="0">
    <tr>
        <th>fecha</th>
        <th>cliente_id</th>
        <th>producto</th>
        <th>cantidad</th>
        <th>precio_unitario</th>
        <th>metodo_pago</th>
    </tr>
    <tr><td>2024-03-01</td><td>C001</td><td>Laptop</td><td>1</td><td>1200</td><td>Tarjeta</td></tr>
    <tr><td>2024-03-01</td><td>C002</td><td>Mouse</td><td>2</td><td>25</td><td>Efectivo</td></tr>
    <tr><td>2024-03-02</td><td>C001</td><td>Teclado</td><td></td><td>50</td><td>Tarjeta</td></tr>
    <tr><td>2024-03-02</td><td>C003</td><td>Monitor</td><td>1</td><td>300</td><td>Transferencia</td></tr>
    <tr><td>2024-03-03</td><td>C002</td><td>Mouse</td><td>1</td><td>25</td><td>Efectivo</td></tr>
    <tr><td>2024-03-03</td><td>C001</td><td>Teclado</td><td>1</td><td></td><td>Tarjeta</td></tr>
    <tr><td>2024-03-03</td><td>C004</td><td>Silla</td><td>1</td><td>100</td><td>Efectivo</td></tr>
    <tr><td>2024-03-01</td><td>C001</td><td>Laptop</td><td>1</td><td>1200</td><td>Tarjeta</td></tr>
</table>

<h3>âœ”ï¸ 1. Cantidad de registros y columnas</h3>
<pre><code>
print(f'Cantidad de registros: {df.shape[0]}')
print(f'Cantidad de columnas: {df.shape[1]}')
</code></pre>
<p><strong>Resultado esperado:</strong><br>
Cantidad de registros: 8<br>
Cantidad de columnas: 6
</p>

<h3>âœ”ï¸ 2. Tipos de datos (numÃ©ricos, categÃ³ricos, fechas)</h3>
<pre><code>
print(df.dtypes)
</code></pre>
<p><strong>Resultado esperado:</strong></p>
<pre>
fecha               object
cliente_id          object
producto            object
cantidad            float64
precio_unitario     float64
metodo_pago         object
</pre>
<p><strong>ObservaciÃ³n:</strong> La columna <code>fecha</code> deberÃ­a ser tipo <code>datetime</code>, por lo que se debe convertir.</p>

<h3>âœ”ï¸ 3. Datos faltantes o nulos</h3>
<pre><code>
print(df.isnull().sum())
</code></pre>
<p><strong>Resultado esperado:</strong></p>
<pre>
fecha              0
cliente_id         0
producto           0
cantidad           1
precio_unitario    1
metodo_pago        0
</pre>
<p><strong>ObservaciÃ³n:</strong> Hay un registro sin <code>cantidad</code> y otro sin <code>precio_unitario</code>, lo cual es crÃ­tico.</p>

<h3>âœ”ï¸ 4. Registros duplicados</h3>
<pre><code>
print(f'Registros duplicados: {df.duplicated().sum()}')
</code></pre>
<p><strong>Resultado esperado:</strong><br>
Registros duplicados: 1</p>
<p><strong>ObservaciÃ³n:</strong> Hay una venta de Laptop duplicada (misma fecha, cliente, producto, etc.).</p>

<h3>âœ”ï¸ 5. Columnas innecesarias</h3>
<p>El anÃ¡lisis de columnas innecesarias es manual y depende del objetivo. 
En este caso, si el <code>metodo_pago</code> no es relevante para el modelo, podrÃ­a eliminarse:</p>
<pre><code>
df = df.drop(columns=['metodo_pago'])
</code></pre>

<h3>Resumen final del chequeo inicial</h3>
<table border="1" cellpadding="5" cellspacing="0">
    <tr>
        <th>Concepto</th>
        <th>Resultado</th>
    </tr>
    <tr><td>Registros</td><td>8</td></tr>
    <tr><td>Columnas</td><td>6</td></tr>
    <tr><td>Tipos de datos</td><td>Algunos incorrectos (fecha como object)</td></tr>
    <tr><td>Valores nulos</td><td>cantidad y precio_unitario</td></tr>
    <tr><td>Duplicados</td><td>1 registro</td></tr>
    <tr><td>Columnas innecesarias</td><td>metodo_pago podrÃ­a eliminarse</td></tr>
</table>

<p>Este anÃ¡lisis inicial es clave para comenzar la limpieza de datos con una visiÃ³n clara de quÃ© problemas existen en la fuente.</p>

<hr>


<pre><code>
df.info()
df.describe()
df.isnull().sum()
df.duplicated().sum()
</code></pre>

<hr>

<hr>

<h2>ğŸ“‹ Buenas PrÃ¡cticas y Recomendaciones al Explorar Datos</h2>

<h3>âœ… Chequeo inicial: El primer diagnÃ³stico</h3>
<p>Piensa en el chequeo inicial como un "check-up mÃ©dico" para tus datos. Antes de entrenar cualquier modelo, 
debes conocer el estado actual de tus datos: su calidad, completitud y posibles inconsistencias. 
Este chequeo es obligatorio en cualquier pipeline de datos serio.</p>

<p>Sin este paso, podrÃ­as entrenar un modelo sobre datos incompletos, desactualizados o corruptos, 
obteniendo resultados poco fiables o directamente errÃ³neos.</p>

<hr>

<h3>âœ… Ejemplos adicionales de columnas innecesarias</h3>
<p>Dependiendo de la naturaleza del proyecto, hay columnas que suelen ser eliminadas porque no aportan valor predictivo:</p>
<ul>
    <li>ğŸ“‘ Notas internas o comentarios.</li>
    <li>ğŸ”— ID de proceso interno (sin relaciÃ³n directa con el target).</li>
    <li>ğŸ‘¤ Usuario que cargÃ³ el registro (irrelevante para el modelo).</li>
</ul>
<p>Recuerda: si una columna no tiene correlaciÃ³n lÃ³gica o estadÃ­stica con la variable objetivo, 
es candidata para ser eliminada.</p>

<hr>

<h3>âœ… RevisiÃ³n especial de fechas y tiempos</h3>
<p>En datos reales, es muy comÃºn que las fechas lleguen mal formateadas o como texto (string). 
Esto dificulta anÃ¡lisis temporales o la creaciÃ³n de features como la edad de un cliente o el tiempo entre eventos.</p>

<pre><code>
df['fecha'] = pd.to_datetime(df['fecha'], errors='coerce')
</code></pre>
<p>AdemÃ¡s, es buena prÃ¡ctica graficar la cantidad de registros por dÃ­a/mes/aÃ±o para detectar:</p>
<ul>
    <li>ğŸ“‰ Gaps de datos (dÃ­as sin registros).</li>
    <li>âš ï¸ Fechas atÃ­picas (aÃ±os fuera de rango: 1900 o 2100).</li>
</ul>

<hr>

<h3>âœ… RevisiÃ³n de cardinalidad en variables categÃ³ricas</h3>
<p>La cardinalidad es la cantidad de valores Ãºnicos en una columna. En variables categÃ³ricas, 
una cardinalidad excesiva es una alerta de datos sucios o de columnas poco relevantes.</p>
<pre><code>
df['producto'].nunique()
</code></pre>
<p>RecomendaciÃ³n: Si tienes una columna como "nombre_producto" con miles de valores Ãºnicos, 
puede ser mejor agruparlos en categorÃ­as mÃ¡s amplias (ej. "ElectrÃ³nica", "Muebles", etc.).</p>

<hr>

<h3>âœ… Documenta los problemas encontrados</h3>
<p>Cada hallazgo es valioso. Documentar quÃ© columnas tenÃ­an nulos, quÃ© fechas estaban fuera de rango 
o quÃ© columnas fueron eliminadas, crea un histÃ³rico Ãºtil para auditorÃ­as o futuros anÃ¡lisis.</p>

<p>Una buena prÃ¡ctica es tener un archivo <strong>data_quality_report.md</strong> donde registres:</p>
<ul>
    <li>âœ… Fecha del anÃ¡lisis.</li>
    <li>âœ… Problemas encontrados (nulos, duplicados, formatos incorrectos).</li>
    <li>âœ… Soluciones aplicadas (imputaciÃ³n, eliminaciÃ³n de registros, etc.).</li>
</ul>

<hr>

<h3>âœ… Automatiza el chequeo inicial</h3>
<p>En proyectos grandes, puedes crear un script estÃ¡ndar (notebook base) que automÃ¡ticamente:</p>
<ul>
    <li>âœ… Carga el dataset.</li>
    <li>âœ… Ejecuta df.info(), df.describe(), df.isnull().sum(), etc.</li>
    <li>âœ… Genera un pequeÃ±o reporte automÃ¡tico (grÃ¡ficas y mÃ©tricas).</li>
</ul>
<p>Esto te ahorra tiempo y asegura que todos los datasets reciban la misma revisiÃ³n de calidad.</p>

<hr>

<h3>ğŸ“Š Ejemplo grÃ¡fico de flujo de trabajo</h3>
<p>Para cerrar, esta es la secuencia ideal al explorar datos:</p>
<ul>
    <li>ğŸ“‚ Carga de datos.</li>
    <li>ğŸ” Chequeo inicial.</li>
    <li>ğŸ“Š VisualizaciÃ³n exploratoria.</li>
    <li>ğŸ’§ Limpieza de datos.</li>
    <li>âš’ï¸ Feature Engineering.</li>
</ul>
<p>Cada paso depende del anterior, creando un flujo de trabajo sÃ³lido y documentado.</p>

<hr>

<h2>ğŸ“š Enlaces recomendados</h2>
<ul>
    <li><a href="https://pandas.pydata.org/docs/" target="_blank">DocumentaciÃ³n oficial de Pandas</a></li>
    <li><a href="https://matplotlib.org/stable/contents.html" target="_blank">DocumentaciÃ³n de Matplotlib</a></li>
    <li><a href="https://seaborn.pydata.org/" target="_blank">DocumentaciÃ³n de Seaborn</a></li>
</ul>
<hr>
<h2>VisualizaciÃ³n exploratoria</h2>
<ul>
    <li>ğŸ“Š Histogramas: distribuciones por variable.</li>
    <li>ğŸ“‰ Boxplots: detecciÃ³n de outliers.</li>
    <li>ğŸ“ˆ Scatter plots: relaciones entre variables.</li>
    <li>ğŸ”¥ Heatmaps: correlaciones entre variables numÃ©ricas.</li>
</ul>

<pre><code>
import matplotlib.pyplot as plt
plt.hist(df['ventas'], bins=20)
plt.title('DistribuciÃ³n de Ventas')
plt.show()
</code></pre>

<hr>

<h2>Errores comunes a identificar</h2>
<ul>
    <li>âš ï¸ Fechas fuera de rango (aÃ±o 1900 o 2100).</li>
    <li>âš ï¸ Valores negativos en columnas como precio o edad.</li>
    <li>âš ï¸ Columnas categÃ³ricas con demasiados valores Ãºnicos (alta cardinalidad).</li>
    <li>âš ï¸ Registros duplicados.</li>
</ul>

<pre><code>
df.duplicated().sum()
</code></pre>

<hr>

<h2>TransformaciÃ³n y Feature Engineering</h2>
<ul>
    <li>âœ… Convertir categorÃ­as a nÃºmeros (encoding).</li>
    <li>âœ… Normalizar (Min-Max) o estandarizar (Z-score).</li>
    <li>âœ… Crear columnas nuevas (combinaciones o ratios).</li>
</ul>

<pre><code>
df['categoria_precio'] = pd.qcut(df['precio'], q=4, labels=['bajo', 'medio', 'alto', 'muy alto'])
</code></pre>

<hr>

<h2>PrÃ¡ctica</h2>
<p>Con el dataset de ventas en S3, realiza:</p>
<ol>
    <li>Cargar datos.</li>
    <li>Revisar estructura, tipos y nulos.</li>
    <li>Graficar distribuciones clave.</li>
    <li>Limpiar duplicados y errores.</li>
    <li>Crear al menos dos variables nuevas.</li>
</ol>

<hr>

<h2>ConclusiÃ³n</h2>
<p>Datos limpios son la clave para modelos precisos. En el prÃ³ximo mÃ³dulo, entrenaremos el primer modelo.</p>
