<h1>M√≥dulo 2: Preparando datos para IA/ML</h1>

<h2>Objetivo</h2>
<p>En este m√≥dulo aprender√°s por qu√© los datos son el alma de cualquier proyecto de Machine Learning, c√≥mo evaluar la calidad de un dataset y qu√© opciones ofrece AWS para crear repositorios y pipelines de datos eficientes. 
Al final, crear√°s tu primer pipeline real de datos en AWS, combinando almacenamiento, ingesta y procesamiento.</p>

<hr>

<h2>¬øQu√© es un buen dataset para IA/ML?</h2>
<p>Un modelo es tan bueno como los datos con los que lo entrenas. Por eso, entender qu√© hace que un dataset sea √∫til es cr√≠tico. Un buen dataset tiene:</p>

<ul>
    <li><strong>Calidad:</strong> Datos sin errores (fechas inv√°lidas, campos vac√≠os, registros duplicados o inconsistentes).</li>
    <li><strong>Volumen:</strong> Suficiente cantidad para representar patrones reales. Un modelo no puede generalizar si tiene muy pocos ejemplos.</li>
    <li><strong>Variedad:</strong> Diversidad de fuentes y formatos, reflejando el entorno real (por ejemplo, combinar datos de ventas con clima).</li>
    <li><strong>Representatividad:</strong> Los datos deben reflejar el fen√≥meno real. Si entrenas un modelo de fraudes solo con datos de un pa√≠s, fallar√° al detectar fraudes globales.</li>
    <li><strong>Actualizaci√≥n:</strong> En casos como detecci√≥n de fraudes o recomendaciones, es clave que los datos est√©n frescos.</li>
</ul>

<p><strong>Recuerda:</strong> Garbage In, Garbage Out. Un mal dataset no puede producir un buen modelo, sin importar cu√°n avanzado sea el algoritmo.</p>

<hr>

<h2>Crear repositorios de datos en AWS</h2>
<p>En AWS, podemos almacenar datos en diferentes servicios, cada uno optimizado para ciertos casos:</p>

<ul>
    <li>‚úÖ <strong>Amazon S3:</strong> Almacenamiento flexible (data lake), perfecto para datos crudos en CSV, JSON, Parquet, im√°genes o videos.</li>
    <li>‚úÖ <strong>Amazon Redshift:</strong> Almac√©n anal√≠tico, optimizado para consultar grandes vol√∫menes de datos estructurados con SQL.</li>
    <li>‚úÖ <strong>Amazon RDS:</strong> Bases de datos relacionales gestionadas (MySQL, PostgreSQL, etc.), ideal para datos transaccionales.</li>
    <li>‚úÖ <strong>AWS Lake Formation:</strong> Servicio para crear un Data Lake completo con control centralizado de permisos.</li>
</ul>

<p><strong>Recomendaci√≥n pr√°ctica:</strong> Para proyectos de IA/ML, la combinaci√≥n m√°s com√∫n es:</p>
<ul>
    <li>üíæ Datos crudos ‚Üí S3</li>
    <li>üî® Datos transformados ‚Üí S3 o Redshift</li>
    <li>üìä Consultas exploratorias ‚Üí Athena o Redshift</li>
</ul>

<p><strong>Diferencia clave:</strong></p>
<ul>
    <li><strong>S3:</strong> Almacenamiento barato y flexible, pero lento para an√°lisis directo.</li>
    <li><strong>Redshift:</strong> R√°pido para consultas anal√≠ticas complejas, pero m√°s costoso y r√≠gido.</li>
    <li><strong>Athena:</strong> Consultas SQL directas sobre datos en S3 (serverless), ideal para an√°lisis ad-hoc.</li>
</ul>

<hr>

<h2>Pipelines de ingesta en AWS</h2>
<p>En la vida real, los datos no llegan "limpitos" a nuestro repositorio. Hay que extraerlos, limpiarlos, transformarlos y cargarlos. En AWS, podemos armar pipelines con:</p>

<ul>
    <li>‚úÖ <strong>AWS Glue:</strong> Servicio ETL serverless, perfecto para transformar grandes vol√∫menes de datos, convertir formatos o enriquecer datasets.</li>
    <li>‚úÖ <strong>Amazon Kinesis:</strong> Ingesta de datos en tiempo real, ideal para streams como logs o clics web.</li>
    <li>‚úÖ <strong>AWS Lambda:</strong> Funciones serverless que se ejecutan cuando llega un archivo a S3, para hacer validaciones o transformaciones ligeras.</li>
</ul>

<p><strong>¬øCu√°ndo usar cada uno?</strong></p>
<ul>
    <li><strong>Glue:</strong> Cuando hay vol√∫menes grandes y transformaciones complejas (unificaci√≥n de formatos, limpieza, joins).</li>
    <li><strong>Kinesis:</strong> Cuando los datos fluyen constantemente (streams en tiempo real, logs, sensores IoT).</li>
    <li><strong>Lambda:</strong> Para automatizar respuestas a eventos (nuevo archivo llega a S3 ‚Üí validarlo y moverlo).</li>
</ul>

<p><strong>Ejemplo t√≠pico:</strong></p>
<pre>
Cliente sube archivo CSV a S3.
Evento S3 dispara Lambda.
Lambda valida el archivo y lanza Glue.
Glue limpia y transforma los datos.
Datos procesados se guardan en un bucket final o en Redshift.
</pre>

<hr>

<h2>Pr√°ctica: Tu primer pipeline de datos en AWS</h2>
<p>Vamos a crear este flujo end-to-end:</p>

<ol>
    <li>Crear un bucket en S3 llamado <code>ml-curso-datos</code>.</li>
    <li>Subir un archivo de muestra (por ejemplo, ventas.csv).</li>
    <li>Configurar un evento de S3 que llame a una Lambda cada vez que llegue un archivo nuevo.</li>
    <li>Crear una Lambda que valide el archivo (estructura, nombres de columnas) y lance un Job de Glue.</li>
    <li>En Glue, crear un script simple que limpia los datos (elimina filas vac√≠as y normaliza formatos).</li>
    <li>Guardar los datos procesados en un bucket final <code>ml-curso-datos-limpiados</code>.</li>
    <li>Opcional: Consultar los datos limpios con Athena.</li>
</ol>

<p><strong>Enlaces √∫tiles:</strong></p>
<ul>
    <li><a href="https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html" target="_blank">Documentaci√≥n AWS Glue</a></li>
    <li><a href="https://docs.aws.amazon.com/lambda/latest/dg/getting-started.html" target="_blank">Gu√≠a inicial AWS Lambda</a></li>
    <li><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html" target="_blank">Introducci√≥n Amazon S3</a></li>
</ul>

<hr>

<h2>Conclusi√≥n</h2>
<p>En Machine Learning, **los datos son m√°s importantes que los algoritmos**. Un pipeline robusto asegura que tus modelos aprendan de informaci√≥n limpia, fresca y relevante.</p>
<p>En el pr√≥ximo m√≥dulo, aprenderemos c√≥mo explorar, visualizar y limpiar esos datos para convertirlos en insumos perfectos para entrenar modelos.</p>

