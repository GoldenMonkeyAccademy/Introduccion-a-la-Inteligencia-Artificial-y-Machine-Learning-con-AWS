<h1>M√≥dulo 2: Preparando datos para IA/ML</h1>

<h2>Objetivo</h2>
<p>En este m√≥dulo aprender√°s por qu√© los datos son el alma de cualquier proyecto de Machine Learning, c√≥mo evaluar la calidad de un dataset y qu√© opciones ofrece AWS para crear repositorios y pipelines de datos eficientes. 
Al final, crear√°s tu primer pipeline real de datos en AWS, combinando almacenamiento, ingesta y procesamiento.</p>

<hr>

<h2>¬øQu√© es un buen dataset para IA/ML?</h2>
<p>Un modelo es tan bueno como los datos con los que lo entrenas. Por eso, entender qu√© hace que un dataset sea √∫til es cr√≠tico. Un buen dataset tiene:</p>

<ul>
    <li><strong>Calidad:</strong> Datos sin errores (fechas inv√°lidas, campos vac√≠os, registros duplicados o inconsistentes).</li>
    <li><strong>Volumen:</strong> Suficiente cantidad para representar patrones reales. Un modelo no puede generalizar si tiene muy pocos ejemplos.</li>
    <li><strong>Variedad:</strong> Diversidad de fuentes y formatos, reflejando el entorno real (por ejemplo, combinar datos de ventas con clima).</li>
    <li><strong>Representatividad:</strong> Los datos deben reflejar el fen√≥meno real. Si entrenas un modelo de fraudes solo con datos de un pa√≠s, fallar√° al detectar fraudes globales.</li>
    <li><strong>Actualizaci√≥n:</strong> En casos como detecci√≥n de fraudes o recomendaciones, es clave que los datos est√©n frescos.</li>
</ul>

<p><strong>Recuerda:</strong> Garbage In, Garbage Out. Un mal dataset no puede producir un buen modelo, sin importar cu√°n avanzado sea el algoritmo.</p>

<hr>

<h2>Crear repositorios de datos en AWS</h2>

<p>En cualquier proyecto de IA/ML, uno de los primeros pasos es decidir d√≥nde almacenar los datos. AWS ofrece m√∫ltiples opciones, cada una dise√±ada para diferentes tipos de datos y necesidades.</p>

<hr>

<h3>üìÇ ¬øQu√© es un Data Lake?</h3>
<p>Un <strong>Data Lake</strong> es un repositorio central donde se almacenan <strong>todos los datos</strong>, sin importar su formato o estructura. Puede contener:</p>
<ul>
    <li>Datos estructurados (tablas CSV, bases relacionales exportadas).</li>
    <li>Datos semi-estructurados (archivos JSON o XML).</li>
    <li>Datos no estructurados (im√°genes, videos, logs).</li>
</ul>
<p>La idea es guardar los datos tal como llegan, sin filtrarlos ni transformarlos de inmediato. Esto permite explorarlos o procesarlos m√°s adelante seg√∫n las necesidades.</p>

<h4>‚úÖ Amazon S3</h4>
<p><strong>Amazon S3</strong> es el servicio principal para construir Data Lakes en AWS. Es un almacenamiento flexible y econ√≥mico donde puedes guardar cualquier tipo de archivo.</p>
<p><strong>¬øPor qu√© es clave para IA/ML?</strong></p>
<ul>
    <li>Compatible con todos los formatos.</li>
    <li>Escalable y econ√≥mico.</li>
    <li>Integrado con Glue, SageMaker, Athena y m√°s.</li>
</ul>

<hr>

<h3>üìä ¬øQu√© es un Almac√©n Anal√≠tico (Data Warehouse)?</h3>
<p>Un <strong>Data Warehouse</strong> es un almac√©n optimizado para <strong>consultas SQL complejas</strong> sobre grandes vol√∫menes de datos <strong>estructurados</strong>.</p>
<p>A diferencia de un Data Lake (que almacena datos crudos), un Data Warehouse requiere que los datos est√©n previamente procesados y organizados en tablas con un esquema fijo.</p>

<h4>‚úÖ Amazon Redshift</h4>
<p><strong>Amazon Redshift</strong> es el almac√©n anal√≠tico de AWS, ideal cuando necesitas:</p>
<ul>
    <li>Analizar datos hist√≥ricos en detalle.</li>
    <li>Crear dashboards o reportes con SQL.</li>
    <li>Realizar agregaciones y an√°lisis exploratorios avanzados antes de entrenar un modelo.</li>
</ul>

<hr>

<h3>üìã ¬øQu√© es una Base de Datos Relacional?</h3>
<p>Una <strong>base de datos relacional</strong> organiza los datos en tablas conectadas entre s√≠ (por ejemplo, una tabla de clientes vinculada a una tabla de pedidos).</p>
<p>Estas bases son ideales para aplicaciones transaccionales, como sistemas de ventas, donde cada registro tiene una relaci√≥n directa con otros registros.</p>

<h4>‚úÖ Amazon RDS</h4>
<p><strong>Amazon RDS</strong> es el servicio gestionado de bases de datos relacionales en AWS (MySQL, PostgreSQL, SQL Server, entre otros).</p>
<p><strong>¬øPor qu√© es relevante para IA/ML?</strong></p>
<ul>
    <li>Si los datos de entrenamiento provienen de un sistema de ventas o un CRM, probablemente est√°n en RDS.</li>
    <li>RDS facilita exportar estos datos a S3 para crear datasets de entrenamiento.</li>
</ul>

<hr>

<h3>üîê ¬øQu√© es la Gobernanza de un Data Lake?</h3>
<p>A medida que el Data Lake crece, es necesario controlar qui√©n accede a qu√© datos, documentar qu√© datasets existen y aplicar pol√≠ticas de seguridad.</p>

<h4>‚úÖ AWS Lake Formation</h4>
<p><strong>AWS Lake Formation</strong> es el servicio que permite:</p>
<ul>
    <li>Definir permisos de acceso a nivel de tabla, columna o fila.</li>
    <li>Crear un cat√°logo central de datasets.</li>
    <li>Asegurar que cada equipo acceda solo a los datos relevantes para su trabajo.</li>
</ul>
<p>Esto es clave en empresas donde el mismo Data Lake es usado por diferentes √°reas: ciencia de datos, finanzas, marketing, etc.</p>

<hr>

<h2>Comparaci√≥n r√°pida</h2>
<table border="1">
<tr>
    <th>Servicio</th>
    <th>Tipo de datos</th>
    <th>Casos de uso</th>
</tr>
<tr>
    <td>Amazon S3</td>
    <td>Cualquier formato</td>
    <td>Data Lake - Almacenamiento flexible y econ√≥mico</td>
</tr>
<tr>
    <td>Amazon Redshift</td>
    <td>Datos estructurados</td>
    <td>Almac√©n anal√≠tico - An√°lisis SQL complejo</td>
</tr>
<tr>
    <td>Amazon RDS</td>
    <td>Datos relacionales</td>
    <td>Base transaccional - Sistemas operacionales</td>
</tr>
<tr>
    <td>AWS Lake Formation</td>
    <td>Metadatos y permisos</td>
    <td>Gobernanza centralizada del Data Lake</td>
</tr>
</table>

<hr>

<h2>Conclusi√≥n</h2>
<p>No existe un √∫nico repositorio "correcto". La clave es entender las fortalezas de cada servicio y combinarlos seg√∫n las necesidades de tu proyecto:</p>
<ul>
    <li>üìÇ <strong>Amazon S3:</strong> Guardar datos crudos o semi-procesados.</li>
    <li>üìä <strong>Amazon Redshift:</strong> Consultar grandes vol√∫menes de datos ya transformados.</li>
    <li>üìã <strong>Amazon RDS:</strong> Manejar operaciones transaccionales (datos vivos).</li>
    <li>üîê <strong>AWS Lake Formation:</strong> Controlar qui√©n accede a qu√©.</li>
</ul>

<p>Con estos conceptos claros, est√°s listo para construir pipelines que no solo mueven datos, sino que garantizan calidad, seguridad y eficiencia.</p>


<hr>

<h2>Pipelines de ingesta en AWS</h2>
<p>En la vida real, los datos no llegan "limpitos" a nuestro repositorio. Hay que extraerlos, limpiarlos, transformarlos y cargarlos. En AWS, podemos armar pipelines con:</p>

<ul>
    <li>‚úÖ <strong>AWS Glue:</strong> Servicio ETL serverless, perfecto para transformar grandes vol√∫menes de datos, convertir formatos o enriquecer datasets.</li>
    <li>‚úÖ <strong>Amazon Kinesis:</strong> Ingesta de datos en tiempo real, ideal para streams como logs o clics web.</li>
    <li>‚úÖ <strong>AWS Lambda:</strong> Funciones serverless que se ejecutan cuando llega un archivo a S3, para hacer validaciones o transformaciones ligeras.</li>
</ul>

<p><strong>¬øCu√°ndo usar cada uno?</strong></p>
<ul>
    <li><strong>Glue:</strong> Cuando hay vol√∫menes grandes y transformaciones complejas (unificaci√≥n de formatos, limpieza, joins).</li>
    <li><strong>Kinesis:</strong> Cuando los datos fluyen constantemente (streams en tiempo real, logs, sensores IoT).</li>
    <li><strong>Lambda:</strong> Para automatizar respuestas a eventos (nuevo archivo llega a S3 ‚Üí validarlo y moverlo).</li>
</ul>

<p><strong>Ejemplo t√≠pico:</strong></p>
<pre>
Cliente sube archivo CSV a S3.
Evento S3 dispara Lambda.
Lambda valida el archivo y lanza Glue.
Glue limpia y transforma los datos.
Datos procesados se guardan en un bucket final o en Redshift.
</pre>

<hr>

<h2>Pr√°ctica: Tu primer pipeline de datos en AWS</h2>
<p>Vamos a crear este flujo end-to-end:</p>

<ol>
    <li>Crear un bucket en S3 llamado <code>ml-curso-datos</code>.</li>
    <li>Subir un archivo de muestra (por ejemplo, ventas.csv).</li>
    <li>Configurar un evento de S3 que llame a una Lambda cada vez que llegue un archivo nuevo.</li>
    <li>Crear una Lambda que valide el archivo (estructura, nombres de columnas) y lance un Job de Glue.</li>
    <li>En Glue, crear un script simple que limpia los datos (elimina filas vac√≠as y normaliza formatos).</li>
    <li>Guardar los datos procesados en un bucket final <code>ml-curso-datos-limpiados</code>.</li>
    <li>Opcional: Consultar los datos limpios con Athena.</li>
</ol>

<p><strong>Enlaces √∫tiles:</strong></p>
<ul>
    <li><a href="https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html" target="_blank">Documentaci√≥n AWS Glue</a></li>
    <li><a href="https://docs.aws.amazon.com/lambda/latest/dg/getting-started.html" target="_blank">Gu√≠a inicial AWS Lambda</a></li>
    <li><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html" target="_blank">Introducci√≥n Amazon S3</a></li>
</ul>

<hr>

<h2>Conclusi√≥n</h2>
<p>En Machine Learning, **los datos son m√°s importantes que los algoritmos**. Un pipeline robusto asegura que tus modelos aprendan de informaci√≥n limpia, fresca y relevante.</p>
<p>En el pr√≥ximo m√≥dulo, aprenderemos c√≥mo explorar, visualizar y limpiar esos datos para convertirlos en insumos perfectos para entrenar modelos.</p>

