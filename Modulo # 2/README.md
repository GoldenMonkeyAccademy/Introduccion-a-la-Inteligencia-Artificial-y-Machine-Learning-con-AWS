<h1>M√≥dulo 2: Preparando datos para IA/ML</h1>

<h2>Objetivo</h2>
<p>En este m√≥dulo aprender√°s por qu√© los datos son el alma de cualquier proyecto de Machine Learning, c√≥mo evaluar la calidad de un dataset y qu√© opciones ofrece AWS para crear repositorios y pipelines de datos eficientes. 
Al final, crear√°s tu primer pipeline real de datos en AWS, combinando almacenamiento, ingesta y procesamiento.</p>

<hr>

<h2>¬øQu√© es un buen dataset para IA/ML?</h2>
<p>Un modelo es tan bueno como los datos con los que lo entrenas. Por eso, entender qu√© hace que un dataset sea √∫til es cr√≠tico. Un buen dataset tiene:</p>

<ul>
    <li><strong>Calidad:</strong> Datos sin errores (fechas inv√°lidas, campos vac√≠os, registros duplicados o inconsistentes).</li>
    <li><strong>Volumen:</strong> Suficiente cantidad para representar patrones reales. Un modelo no puede generalizar si tiene muy pocos ejemplos.</li>
    <li><strong>Variedad:</strong> Diversidad de fuentes y formatos, reflejando el entorno real (por ejemplo, combinar datos de ventas con clima).</li>
    <li><strong>Representatividad:</strong> Los datos deben reflejar el fen√≥meno real. Si entrenas un modelo de fraudes solo con datos de un pa√≠s, fallar√° al detectar fraudes globales.</li>
    <li><strong>Actualizaci√≥n:</strong> En casos como detecci√≥n de fraudes o recomendaciones, es clave que los datos est√©n frescos.</li>
</ul>

<p><strong>Recuerda:</strong> Garbage In, Garbage Out. Un mal dataset no puede producir un buen modelo, sin importar cu√°n avanzado sea el algoritmo.</p>

<hr>

<h2>Crear repositorios de datos en AWS</h2>

<p>En cualquier proyecto de IA/ML, uno de los primeros pasos es decidir d√≥nde almacenar los datos. AWS ofrece m√∫ltiples opciones, cada una dise√±ada para diferentes tipos de datos y necesidades.</p>

<hr>

<h3>üìÇ ¬øQu√© es un Data Lake?</h3>
<p>Un <strong>Data Lake</strong> es un repositorio central donde se almacenan <strong>todos los datos</strong>, sin importar su formato o estructura. Puede contener:</p>
<ul>
    <li>Datos estructurados (tablas CSV, bases relacionales exportadas).</li>
    <li>Datos semi-estructurados (archivos JSON o XML).</li>
    <li>Datos no estructurados (im√°genes, videos, logs).</li>
</ul>
<p>La idea es guardar los datos tal como llegan, sin filtrarlos ni transformarlos de inmediato. Esto permite explorarlos o procesarlos m√°s adelante seg√∫n las necesidades.</p>

<h2>Amazon S3</h2>

<p><strong>Amazon S3</strong> (Simple Storage Service) es el servicio de almacenamiento m√°s usado en AWS y es clave para proyectos de Machine Learning porque act√∫a como el repositorio central de datos. En un Data Lake, S3 guarda archivos crudos, transformados, modelos entrenados y artefactos de ML.</p>

<h3>¬øPor qu√© S3 es ideal para IA/ML?</h3>
<ul>
    <li>‚úÖ <strong>Flexibilidad:</strong> Soporta m√∫ltiples formatos: CSV, JSON, Parquet, ORC, im√°genes, audio y video.</li>
    <li>‚úÖ <strong>Escalabilidad:</strong> Crece autom√°ticamente sin l√≠mites de tama√±o.</li>
    <li>‚úÖ <strong>Integraci√≥n:</strong> Compatible con Glue, Athena, SageMaker y otros servicios de datos y ML.</li>
</ul>

<hr>

<h2>Clases de Almacenamiento en Amazon S3</h2>
<p>No todos los datos tienen el mismo patr√≥n de acceso. Por eso, Amazon S3 ofrece m√∫ltiples <strong>clases de almacenamiento</strong>, cada una optimizada para un caso espec√≠fico.</p>

<table border="1" cellpadding="5" cellspacing="0">
    <thead>
        <tr>
            <th>Clase</th>
            <th>Descripci√≥n</th>
            <th>Casos de uso recomendados</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><strong>S3 Standard</strong></td>
            <td>Almacenamiento de alta disponibilidad y durabilidad para datos accedidos frecuentemente.</td>
            <td>Datos operacionales, datasets activos, logs recientes.</td>
        </tr>
        <tr>
            <td><strong>S3 Intelligent-Tiering</strong></td>
            <td>Mueve autom√°ticamente datos entre acceso frecuente e infrecuente seg√∫n patrones de acceso.</td>
            <td>Datos con patrones de acceso variables o desconocidos.</td>
        </tr>
        <tr>
            <td><strong>S3 Standard-IA</strong></td>
            <td>Almacenamiento econ√≥mico para datos accedidos espor√°dicamente.</td>
            <td>Backups de datos recientes, reportes hist√≥ricos.</td>
        </tr>
        <tr>
            <td><strong>S3 One Zone-IA</strong></td>
            <td>Similar a Standard-IA, pero almacenado en una sola AZ (zona de disponibilidad).</td>
            <td>Datos f√°cilmente reproducibles o no cr√≠ticos, copias secundarias.</td>
        </tr>
        <tr>
            <td><strong>S3 Glacier Instant Retrieval</strong></td>
            <td>Archivado econ√≥mico con acceso casi inmediato (milisegundos).</td>
            <td>Archivos hist√≥ricos de acceso ocasional que a√∫n requieren baja latencia.</td>
        </tr>
        <tr>
            <td><strong>S3 Glacier Flexible Retrieval</strong></td>
            <td>Archivado profundo, recuperaci√≥n flexible (minutos u horas).</td>
            <td>Backups a largo plazo, cumplimiento normativo.</td>
        </tr>
        <tr>
            <td><strong>S3 Glacier Deep Archive</strong></td>
            <td>La opci√≥n m√°s barata para archivado a muy largo plazo (retrieval en horas).</td>
            <td>Archivos regulatorios, auditor√≠as hist√≥ricas, data fr√≠a.</td>
        </tr>
    </tbody>
</table>

<hr>
<p><strong>Nota:</strong> Puedes combinar estas clases dentro de un mismo bucket usando <strong>Lifecycle Rules</strong> para mover autom√°ticamente los objetos conforme envejecen.</p>


<hr>

<h2>Lifecycle Rules: Reglas Autom√°ticas de Gesti√≥n</h2>
<p>Con las <strong>Lifecycle Rules</strong>, puedes automatizar el movimiento de datos entre clases, por ejemplo:</p>
<ul>
    <li>üîÑ Mover archivos crudos a Glacier despu√©s de 90 d√≠as.</li>
    <li>üóëÔ∏è Eliminar archivos temporales despu√©s de 30 d√≠as.</li>
</ul>
<p>Estas reglas son clave para controlar costos en proyectos de IA donde los datasets pueden ser enormes.</p>

<hr>

<h2>Pol√≠ticas de Acceso (Bucket Policies)</h2>
<p>La seguridad es fundamental. Con las <strong>Bucket Policies</strong>, defines qui√©n puede leer o escribir en tus buckets y bajo qu√© condiciones.</p>
<p><strong>Ejemplos:</strong></p>
<ul>
    <li>üîí Solo SageMaker puede leer el bucket de entrenamiento.</li>
    <li>üîí Solo Glue puede escribir en el bucket de datos limpios.</li>
    <li>üåç Permitir acceso p√∫blico solo a ciertos archivos (cat√°logos o documentaci√≥n).</li>
</ul>
<p>Las pol√≠ticas se definen en JSON y permiten granularidad total.</p>

<hr>

<h2>Cifrado (Encryption)</h2>
<p>S3 ofrece varias opciones para cifrar datos en reposo:</p>
<ul>
    <li>‚úÖ <strong>SSE-S3:</strong> AWS gestiona las claves por ti.</li>
    <li>‚úÖ <strong>SSE-KMS:</strong> Usa AWS KMS para control total sobre claves.</li>
    <li>‚úÖ <strong>Client-side encryption:</strong> T√∫ mismo cifras los datos antes de subirlos.</li>
</ul>
<p>Adem√°s, puedes habilitar <strong>Default Encryption</strong> para que cualquier archivo subido se cifre autom√°ticamente.</p>

<hr>

<h2>VPC Endpoints</h2>
<p>Si trabajas desde una VPC privada, puedes habilitar un <strong>VPC Endpoint para S3</strong>. Esto permite que el tr√°fico de datos fluya directamente dentro de la red de AWS sin pasar por Internet p√∫blica, mejorando:</p>
<ul>
    <li>‚úÖ Seguridad (menos exposici√≥n).</li>
    <li>‚úÖ Rendimiento (latencias menores).</li>
    <li>‚úÖ Costos (menos uso de NAT Gateways).</li>
</ul>

<hr>

<h2>Manos a la Obra (Hands On)</h2>
<p>Como pr√°ctica, se recomienda hacer lo siguiente:</p>
<ol>
    <li>Crear un bucket en S3 y entender sus opciones de configuraci√≥n (clase, versi√≥n, reglas de lifecycle).</li>
    <li>Subir un archivo CSV y observar su URL p√∫blica (si es permitido).</li>
    <li>Configurar una bucket policy que solo permita acceso desde tu cuenta.</li>
    <li>Configurar Default Encryption usando KMS.</li>
    <li>Si tienes una VPC, configurar un Endpoint para S3 y probar la conexi√≥n.</li>
</ol>

<hr>

<h2>Enlaces Relevantes</h2>
<ul>
    <li><a href="https://docs.aws.amazon.com/s3/" target="_blank">Documentaci√≥n oficial de Amazon S3</a></li>
    <li><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html" target="_blank">Clases de almacenamiento</a></li>
    <li><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html" target="_blank">Lifecycle Rules</a></li>
    <li><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-policy-alternatives-guidelines.html" target="_blank">Bucket Policies</a></li>
    <li><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html" target="_blank">Cifrado en S3</a></li>
    <li><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html" target="_blank">VPC Endpoints para S3</a></li>
</ul>

<hr>

<h2>Conclusi√≥n</h2>
<p>Amazon S3 es mucho m√°s que un simple almacenamiento: es la base para un Data Lake moderno en AWS, con controles de seguridad avanzados, optimizaci√≥n de costos autom√°tica y compatibilidad directa con los principales servicios de Machine Learning como SageMaker, Glue y Athena.</p>


<hr>

<h3>üìä ¬øQu√© es un Almac√©n Anal√≠tico (Data Warehouse)?</h3>
<p>Un <strong>Data Warehouse</strong> es un almac√©n optimizado para <strong>consultas SQL complejas</strong> sobre grandes vol√∫menes de datos <strong>estructurados</strong>.</p>
<p>A diferencia de un Data Lake (que almacena datos crudos), un Data Warehouse requiere que los datos est√©n previamente procesados y organizados en tablas con un esquema fijo.</p>

<h4>‚úÖ Amazon Redshift</h4>
<p><strong>Amazon Redshift</strong> es el almac√©n anal√≠tico de AWS, ideal cuando necesitas:</p>
<ul>
    <li>Analizar datos hist√≥ricos en detalle.</li>
    <li>Crear dashboards o reportes con SQL.</li>
    <li>Realizar agregaciones y an√°lisis exploratorios avanzados antes de entrenar un modelo.</li>
</ul>

<hr>

<h3>üìã ¬øQu√© es una Base de Datos Relacional?</h3>
<p>Una <strong>base de datos relacional</strong> organiza los datos en tablas conectadas entre s√≠ (por ejemplo, una tabla de clientes vinculada a una tabla de pedidos).</p>
<p>Estas bases son ideales para aplicaciones transaccionales, como sistemas de ventas, donde cada registro tiene una relaci√≥n directa con otros registros.</p>

<h4>‚úÖ Amazon RDS</h4>
<p><strong>Amazon RDS</strong> es el servicio gestionado de bases de datos relacionales en AWS (MySQL, PostgreSQL, SQL Server, entre otros).</p>
<p><strong>¬øPor qu√© es relevante para IA/ML?</strong></p>
<ul>
    <li>Si los datos de entrenamiento provienen de un sistema de ventas o un CRM, probablemente est√°n en RDS.</li>
    <li>RDS facilita exportar estos datos a S3 para crear datasets de entrenamiento.</li>
</ul>

<hr>

<h3>üîê ¬øQu√© es la Gobernanza de un Data Lake?</h3>
<p>A medida que el Data Lake crece, es necesario controlar qui√©n accede a qu√© datos, documentar qu√© datasets existen y aplicar pol√≠ticas de seguridad.</p>

<h4>‚úÖ AWS Lake Formation</h4>
<p><strong>AWS Lake Formation</strong> es el servicio que permite:</p>
<ul>
    <li>Definir permisos de acceso a nivel de tabla, columna o fila.</li>
    <li>Crear un cat√°logo central de datasets.</li>
    <li>Asegurar que cada equipo acceda solo a los datos relevantes para su trabajo.</li>
</ul>
<p>Esto es clave en empresas donde el mismo Data Lake es usado por diferentes √°reas: ciencia de datos, finanzas, marketing, etc.</p>

<hr>

<h2>Comparaci√≥n r√°pida</h2>
<table border="1">
<tr>
    <th>Servicio</th>
    <th>Tipo de datos</th>
    <th>Casos de uso</th>
</tr>
<tr>
    <td>Amazon S3</td>
    <td>Cualquier formato</td>
    <td>Data Lake - Almacenamiento flexible y econ√≥mico</td>
</tr>
<tr>
    <td>Amazon Redshift</td>
    <td>Datos estructurados</td>
    <td>Almac√©n anal√≠tico - An√°lisis SQL complejo</td>
</tr>
<tr>
    <td>Amazon RDS</td>
    <td>Datos relacionales</td>
    <td>Base transaccional - Sistemas operacionales</td>
</tr>
<tr>
    <td>AWS Lake Formation</td>
    <td>Metadatos y permisos</td>
    <td>Gobernanza centralizada del Data Lake</td>
</tr>
</table>

<hr>

<h2>Conclusi√≥n</h2>
<p>No existe un √∫nico repositorio "correcto". La clave es entender las fortalezas de cada servicio y combinarlos seg√∫n las necesidades de tu proyecto:</p>
<ul>
    <li>üìÇ <strong>Amazon S3:</strong> Guardar datos crudos o semi-procesados.</li>
    <li>üìä <strong>Amazon Redshift:</strong> Consultar grandes vol√∫menes de datos ya transformados.</li>
    <li>üìã <strong>Amazon RDS:</strong> Manejar operaciones transaccionales (datos vivos).</li>
    <li>üîê <strong>AWS Lake Formation:</strong> Controlar qui√©n accede a qu√©.</li>
</ul>

<p>Con estos conceptos claros, est√°s listo para construir pipelines que no solo mueven datos, sino que garantizan calidad, seguridad y eficiencia.</p>


<hr>

<h2>Pipelines de ingesta en AWS</h2>
<p>En la vida real, los datos no llegan "limpitos" a nuestro repositorio. Hay que extraerlos, limpiarlos, transformarlos y cargarlos. En AWS, podemos armar pipelines con:</p>

<ul>
    <li>‚úÖ <strong>AWS Glue:</strong> Servicio ETL serverless, perfecto para transformar grandes vol√∫menes de datos, convertir formatos o enriquecer datasets.</li>
    <li>‚úÖ <strong>Amazon Kinesis:</strong> Ingesta de datos en tiempo real, ideal para streams como logs o clics web.</li>
    <li>‚úÖ <strong>AWS Lambda:</strong> Funciones serverless que se ejecutan cuando llega un archivo a S3, para hacer validaciones o transformaciones ligeras.</li>
</ul>

<p><strong>¬øCu√°ndo usar cada uno?</strong></p>
<ul>
    <li><strong>Glue:</strong> Cuando hay vol√∫menes grandes y transformaciones complejas (unificaci√≥n de formatos, limpieza, joins).</li>
    <li><strong>Kinesis:</strong> Cuando los datos fluyen constantemente (streams en tiempo real, logs, sensores IoT).</li>
    <li><strong>Lambda:</strong> Para automatizar respuestas a eventos (nuevo archivo llega a S3 ‚Üí validarlo y moverlo).</li>
</ul>

<p><strong>Ejemplo t√≠pico:</strong></p>
<pre>
Cliente sube archivo CSV a S3.
Evento S3 dispara Lambda.
Lambda valida el archivo y lanza Glue.
Glue limpia y transforma los datos.
Datos procesados se guardan en un bucket final o en Redshift.
</pre>

<hr>

<h2>Pr√°ctica: Tu primer pipeline de datos en AWS</h2>
<p>Vamos a crear este flujo end-to-end:</p>

<ol>
    <li>Crear un bucket en S3 llamado <code>ml-curso-datos</code>.</li>
    <li>Subir un archivo de muestra (por ejemplo, ventas.csv).</li>
    <li>Configurar un evento de S3 que llame a una Lambda cada vez que llegue un archivo nuevo.</li>
    <li>Crear una Lambda que valide el archivo (estructura, nombres de columnas) y lance un Job de Glue.</li>
    <li>En Glue, crear un script simple que limpia los datos (elimina filas vac√≠as y normaliza formatos).</li>
    <li>Guardar los datos procesados en un bucket final <code>ml-curso-datos-limpiados</code>.</li>
    <li>Opcional: Consultar los datos limpios con Athena.</li>
</ol>

<p><strong>Enlaces √∫tiles:</strong></p>
<ul>
    <li><a href="https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html" target="_blank">Documentaci√≥n AWS Glue</a></li>
    <li><a href="https://docs.aws.amazon.com/lambda/latest/dg/getting-started.html" target="_blank">Gu√≠a inicial AWS Lambda</a></li>
    <li><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html" target="_blank">Introducci√≥n Amazon S3</a></li>
</ul>

<hr>

<h2>Conclusi√≥n</h2>
<p>En Machine Learning, **los datos son m√°s importantes que los algoritmos**. Un pipeline robusto asegura que tus modelos aprendan de informaci√≥n limpia, fresca y relevante.</p>
<p>En el pr√≥ximo m√≥dulo, aprenderemos c√≥mo explorar, visualizar y limpiar esos datos para convertirlos en insumos perfectos para entrenar modelos.</p>

