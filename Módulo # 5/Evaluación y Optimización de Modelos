<h1>M√≥dulo 5: Evaluaci√≥n y Optimizaci√≥n de Modelos</h1>

<h2>Objetivo</h2>
<p>En este m√≥dulo aprender√°s a medir el desempe√±o real de un modelo entrenado, interpretar correctamente sus m√©tricas y aplicar t√©cnicas de optimizaci√≥n 
para ajustar sus hiperpar√°metros y mejorar su precisi√≥n y robustez. Un buen modelo no es solo el que "aprende", sino el que <strong>generaliza bien</strong> a nuevos datos.</p>

<hr>

<h2>¬øPor qu√© es clave evaluar correctamente?</h2>
<p>Un modelo puede funcionar muy bien sobre los datos de entrenamiento, pero eso no garantiza que funcione bien sobre datos nuevos (datos reales en producci√≥n). 
De ah√≠ la importancia de evaluar con un conjunto de prueba o validaci√≥n que el modelo nunca vio durante el entrenamiento.</p>

<p><strong>Evaluar correctamente evita:</strong></p>
<ul>
    <li>‚úÖ Sobreajuste (overfitting): el modelo memoriza en vez de aprender patrones generales.</li>
    <li>‚úÖ Subajuste (underfitting): el modelo es demasiado simple y no captura la complejidad real.</li>
    <li>‚úÖ M√©tricas enga√±osas: usar la m√©trica equivocada puede hacerte pensar que el modelo es bueno cuando no lo es.</li>
</ul>

<hr>

<h2>M√©tricas clave por tipo de problema</h2>
<p>Cada tipo de problema tiene m√©tricas espec√≠ficas:</p>

<h3>üìÇ Clasificaci√≥n (predecir categor√≠as)</h3>
<ul>
    <li><strong>Accuracy:</strong> porcentaje total de predicciones correctas.</li>
    <li><strong>Precision:</strong> de las predicciones positivas, ¬øcu√°ntas eran realmente positivas?</li>
    <li><strong>Recall (Sensibilidad):</strong> de los casos positivos reales, ¬øcu√°ntos detect√≥ el modelo?</li>
    <li><strong>F1 Score:</strong> balance entre precisi√≥n y recall.</li>
    <li><strong>ROC-AUC:</strong> mide qu√© tan bien el modelo separa las clases.</li>
</ul>

<pre><code>
from sklearn.metrics import classification_report, roc_auc_score

print(classification_report(y_test, y_pred))
print(f'ROC-AUC: {roc_auc_score(y_test, y_prob)}')
</code></pre>

<h3>üìä Regresi√≥n (predecir valores num√©ricos)</h3>
<ul>
    <li><strong>RMSE (Root Mean Squared Error):</strong> error promedio cuadr√°tico (da m√°s peso a errores grandes).</li>
    <li><strong>MAE (Mean Absolute Error):</strong> error promedio absoluto (m√°s interpretativo).</li>
    <li><strong>R¬≤ (Coeficiente de determinaci√≥n):</strong> porcentaje de la varianza explicada por el modelo.</li>
</ul>

<pre><code>
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

print(f'RMSE: {mean_squared_error(y_test, y_pred, squared=False)}')
print(f'MAE: {mean_absolute_error(y_test, y_pred)}')
print(f'R¬≤: {r2_score(y_test, y_pred)}')
</code></pre>

<hr>

<h2>¬øQu√© es la optimizaci√≥n de hiperpar√°metros?</h2>
<p>Entrenar un modelo implica ajustar sus <strong>par√°metros internos</strong> (por ejemplo, coeficientes de una regresi√≥n). 
Pero cada algoritmo tambi√©n tiene <strong>hiperpar√°metros</strong> externos, como:</p>
<ul>
    <li>‚úÖ Profundidad de un √°rbol.</li>
    <li>‚úÖ Tasa de aprendizaje.</li>
    <li>‚úÖ N√∫mero de √°rboles en un Random Forest.</li>
</ul>

<p>Elegir buenos hiperpar√°metros puede marcar la diferencia entre un modelo mediocre y un modelo de clase mundial.</p>

<hr>

<h2>Enfoques para optimizaci√≥n de hiperpar√°metros</h2>
<h3>1Ô∏è‚É£ B√∫squeda Manual (Grid Search)</h3>
<p>Definimos combinaciones fijas y entrenamos el modelo con cada una. Es simple, pero puede ser lento y costoso.</p>

<h3>2Ô∏è‚É£ B√∫squeda Aleatoria (Random Search)</h3>
<p>Probamos combinaciones al azar dentro de un rango. Es m√°s eficiente que grid search, pero no garantiza encontrar la mejor combinaci√≥n.</p>

<h3>3Ô∏è‚É£ Optimizaci√≥n Bayesiana (SageMaker Automatic Tuning)</h3>
<p>Usa un enfoque probabil√≠stico que aprende qu√© combinaciones funcionan mejor y explora de forma inteligente.</p>

<hr>

<h2>Optimizaci√≥n autom√°tica en SageMaker</h2>
<p>En AWS SageMaker, puedes crear un <strong>Hyperparameter Tuning Job</strong> que entrena autom√°ticamente m√∫ltiples modelos, 
probando diferentes combinaciones de hiperpar√°metros y eligiendo la mejor.</p>

<h3>Ejemplo en c√≥digo:</h3>
<pre><code>
from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter

# Rango de hiperpar√°metros
hyperparameter_ranges = {
    'eta': ContinuousParameter(0.01, 0.3),
    'max_depth': IntegerParameter(3, 10),
    'subsample': ContinuousParameter(0.5, 1.0)
}

# Tuning Job
tuner = HyperparameterTuner(
    estimator=xgb_estimator,
    objective_metric_name='validation:auc',
    hyperparameter_ranges=hyperparameter_ranges,
    max_jobs=10,
    max_parallel_jobs=2
)

# Ejecutar tuning
tuner.fit({'train': train_path, 'validation': validation_path})
</code></pre>

<p>Este proceso es <strong>serverless</strong>, lo cual significa que SageMaker provisiona autom√°ticamente las instancias necesarias 
y te cobra solo por el tiempo de ejecuci√≥n.</p>

<hr>

<h2>Comparar modelo inicial vs modelo optimizado</h2>
<p>Al final, comparas el desempe√±o del modelo original (sin tuning) vs el modelo final optimizado. 
Idealmente, deber√≠as ver:</p>
<ul>
    <li>‚úÖ Mejor precisi√≥n o menor error.</li>
    <li>‚úÖ Mejor estabilidad (menor varianza en resultados al probar con nuevos datos).</li>
    <li>‚úÖ Menor tiempo de inferencia (si optimizaste hiperpar√°metros de eficiencia).</li>
</ul>

<pre><code>
# Comparar m√©tricas
print(f'M√©tricas iniciales: {base_metrics}')
print(f'M√©tricas optimizadas: {tuned_metrics}')
</code></pre>

<hr>

<h2>Pr√°ctica recomendada</h2>
<ol>
    <li>‚úÖ Entrenar un modelo base sin optimizaci√≥n.</li>
    <li>‚úÖ Evaluarlo usando m√©tricas apropiadas.</li>
    <li>‚úÖ Ejecutar un Hyperparameter Tuning Job en SageMaker.</li>
    <li>‚úÖ Comparar desempe√±o antes y despu√©s del tuning.</li>
    <li>‚úÖ Documentar el impacto de cada hiperpar√°metro.</li>
</ol>

<hr>

<h2>Enlaces √∫tiles</h2>
<ul>
    <li><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics" target="_blank">M√©tricas en scikit-learn</a></li>
    <li><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html" target="_blank">Tuning Autom√°tico en SageMaker</a></li>
    <li><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html" target="_blank">Ejemplo de Tuning con XGBoost</a></li>
</ul>

<hr>

<h2>Conclusi√≥n</h2>
<p>Un modelo bien optimizado puede superar ampliamente un modelo entrenado sin tuning. 
Adem√°s, evaluar correctamente te permite detectar problemas como bias, sobreajuste o datos desbalanceados. 
En el pr√≥ximo m√≥dulo, llevaremos nuestro modelo optimizado a producci√≥n.</p>
