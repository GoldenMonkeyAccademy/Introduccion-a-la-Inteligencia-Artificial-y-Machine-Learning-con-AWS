<h1>MÃ³dulo 3: ExploraciÃ³n y Limpieza de Datos para Machine Learning</h1>

<h2>Objetivo</h2>
<p>En este mÃ³dulo aprenderÃ¡s a explorar, entender y limpiar datos usando herramientas de AWS y Python. 
Este proceso es crucial para cualquier proyecto de Machine Learning, ya que la calidad de tus datos define directamente el desempeÃ±o de tu modelo.</p>

<hr>

<h2>Â¿Por quÃ© es clave la exploraciÃ³n de datos?</h2>
<p>Entrenar un modelo sin revisar tus datos es como cocinar sin leer los ingredientes: 
puede funcionar, pero lo mÃ¡s probable es que salga mal. 
La exploraciÃ³n de datos permite:</p>
<ul>
    <li>âœ… Detectar datos faltantes, duplicados o inconsistentes.</li>
    <li>âœ… Conocer las distribuciones de cada variable.</li>
    <li>âœ… Identificar valores extremos (outliers).</li>
    <li>âœ… Descubrir relaciones entre variables (correlaciones).</li>
    <li>âœ… Evaluar la necesidad de normalizar, escalar o transformar variables.</li>
</ul>

<p><strong>Herramientas clave:</strong> Pandas, Matplotlib, Seaborn y Amazon Athena. Combinaremos lo mejor de Python con el poder de AWS.</p>

<hr>

<h2>Primer paso: Cargar datos en un Notebook</h2>
<p>Para explorar los datos, primero hay que cargarlos. En AWS, hay varias opciones:</p>
<ul>
    <li>ğŸ“‚ Leer directamente desde un archivo CSV en S3 con Pandas.</li>
    <li>ğŸ“Š Consultar datos directamente desde Redshift o Athena (SQL en la nube).</li>
    <li>ğŸ”— Usar SageMaker Data Wrangler, que facilita cargar, transformar y visualizar datos.</li>
</ul>

<h3>Ejemplo bÃ¡sico: Leer un CSV desde S3</h3>
<pre><code>
import pandas as pd
import boto3

s3 = boto3.client('s3')
response = s3.get_object(Bucket='ml-curso-datos-limpiados', Key='ventas_procesadas.csv')
df = pd.read_csv(response['Body'])

df.head()
</code></pre>

<hr>

<h2>Chequeo inicial: Evaluar la calidad de los datos</h2>
<p>Una vez cargados los datos, comienza el chequeo inicial:</p>
<ul>
    <li>âœ”ï¸ Â¿CuÃ¡ntas filas y columnas hay?</li>
    <li>âœ”ï¸ Â¿CuÃ¡les son los tipos de datos (numÃ©ricos, categÃ³ricos, fechas)?</li>
    <li>âœ”ï¸ Â¿CuÃ¡ntos valores faltantes hay?</li>
    <li>âœ”ï¸ Â¿Existen registros duplicados?</li>
    <li>âœ”ï¸ Â¿Hay columnas irrelevantes?</li>
</ul>

<h3>Comandos clave:</h3>
<pre><code>
df.info()
df.describe()
df.isnull().sum()
df.duplicated().sum()
</code></pre>

<hr>

<h2>VisualizaciÃ³n exploratoria</h2>
<p>Las grÃ¡ficas permiten detectar patrones, distribuciones extraÃ±as y outliers visualmente.</p>
<ul>
    <li>ğŸ“Š Histogramas: distribuciones por variable.</li>
    <li>ğŸ“‰ Boxplots: detecciÃ³n de outliers.</li>
    <li>ğŸ“ˆ Scatter plots: relaciones entre variables.</li>
    <li>ğŸ”¥ Heatmaps: correlaciones entre variables numÃ©ricas.</li>
</ul>

<h3>Ejemplo: Histograma de ventas</h3>
<pre><code>
import matplotlib.pyplot as plt

plt.hist(df['ventas'], bins=20)
plt.title('DistribuciÃ³n de Ventas')
plt.show()
</code></pre>

<hr>

<h2>Errores comunes a identificar</h2>
<p>Al explorar, es comÃºn encontrar:</p>
<ul>
    <li>âš ï¸ Fechas fuera de rango (1900 o 2100).</li>
    <li>âš ï¸ Valores negativos en variables que no deberÃ­an tenerlos (precio, edad).</li>
    <li>âš ï¸ Columnas categÃ³ricas con demasiados valores Ãºnicos (cardinalidad alta).</li>
    <li>âš ï¸ Registros duplicados (mismo cliente y misma fecha).</li>
</ul>

<h3>Detectar duplicados:</h3>
<pre><code>
df.duplicated().sum()
</code></pre>

<hr>

<h2>Limpieza de datos</h2>
<p>Una vez identificados los problemas, es hora de limpiar:</p>
<ul>
    <li>ğŸ’§ Eliminar columnas irrelevantes o redundantes.</li>
    <li>ğŸ’§ Completar o eliminar datos faltantes.</li>
    <li>ğŸ’§ Corregir tipos de datos (fechas como datetime, categorÃ­as como strings).</li>
    <li>ğŸ’§ Crear nuevas variables Ãºtiles (feature engineering).</li>
</ul>

<h3>Ejemplos:</h3>
<pre><code>
# Eliminar columna innecesaria
df = df.drop(columns=['columna_inutil'])

# Llenar nulos con la mediana
df['precio'].fillna(df['precio'].median(), inplace=True)

# Crear nueva variable
df['precio_m2'] = df['precio'] / df['metros_cuadrados']
</code></pre>

<hr>

<h2>TransformaciÃ³n y Feature Engineering</h2>
<p>La limpieza no es solo corregir errores. TambiÃ©n es preparar los datos para que los modelos trabajen mejor.</p>
<ul>
    <li>âœ… Convertir categorÃ­as en nÃºmeros (encoding).</li>
    <li>âœ… Normalizar o escalar valores.</li>
    <li>âœ… Crear nuevas variables (ej. ratios o combinaciones de otras).</li>
</ul>

<h3>Ejemplo: Crear una columna categÃ³rica</h3>
<pre><code>
df['categoria_precio'] = pd.qcut(df['precio'], q=4, labels=['bajo', 'medio', 'alto', 'muy alto'])
</code></pre>

<hr>

<h2>PrÃ¡ctica: ExploraciÃ³n y limpieza real</h2>
<p>Te toca poner manos a la obra. Usaremos el dataset de ventas subido a S3 en el MÃ³dulo 2. 
Tu reto es completar los siguientes pasos:</p>
<ol>
    <li>ğŸ“‚ Leer dataset desde S3.</li>
    <li>ğŸ“Š Revisar estructura, tipos y valores nulos.</li>
    <li>ğŸ“ˆ Graficar distribuciones y relaciones clave.</li>
    <li>ğŸ’§ Limpiar datos inconsistentes.</li>
    <li>âš’ï¸ Crear al menos dos nuevas variables derivadas.</li>
</ol>

<h3>Enlaces Ãºtiles</h3>
<ul>
    <li><a href="https://pandas.pydata.org/docs/" target="_blank">DocumentaciÃ³n oficial de Pandas</a></li>
    <li><a href="https://matplotlib.org/stable/contents.html" target="_blank">DocumentaciÃ³n oficial de Matplotlib</a></li>
    <li><a href="https://seaborn.pydata.org/" target="_blank">DocumentaciÃ³n oficial de Seaborn</a></li>
    <li><a href="https://docs.aws.amazon.com/athena/latest/ug/what-is.html" target="_blank">IntroducciÃ³n a Amazon Athena</a></li>
</ul>

<hr>

<h2>ConclusiÃ³n</h2>
<p>En Machine Learning, el 80% del trabajo es preparar datos. 
Un buen anÃ¡lisis exploratorio te ayuda a:</p>
<ul>
    <li>âœ… Entender el negocio desde los datos.</li>
    <li>âœ… Detectar problemas antes de entrenar modelos.</li>
    <li>âœ… Documentar el estado inicial de la data (para auditorÃ­a y trazabilidad).</li>
    <li>âœ… Crear features valiosas que mejoren el desempeÃ±o del modelo.</li>
</ul>
<p>En el prÃ³ximo mÃ³dulo, con datos limpios y enriquecidos, seleccionaremos algoritmos y entrenaremos nuestro primer modelo de Machine Learning.</p>
